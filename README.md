# Ultra Light Live Camera Vision Language System using BLIP

# Abstract

This project presents a lightweight real-time Vision-Language Model (VLM) system for live scene understanding using a webcam stream. The system leverages the BLIP (Bootstrapping Language-Image Pretraining) image captioning model to generate natural language descriptions from live camera frames.

Unlike large-scale multimodal systems requiring high-end GPUs, this implementation is optimized for CPU-based inference, making it suitable for low-resource environments, edge devices, and academic experimentation.

The architecture emphasizes efficient preprocessing, background inference threading, and structured UI rendering to ensure smooth real-time interaction.
